{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习项目作业\n",
    "本文尝试对数据集进行研究，并使用机器学习的方法，挖掘可能存在的嫌疑犯。\n",
    "\n",
    "## 数据清洗\n",
    "首先，我引入本次研究所使用的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/newchama/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    my_dataset = data_dict\n",
    "    features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'director_fees',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi']\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "print len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除非人数据点\n",
    "首先，打印数据所有人的名称，去除明显非人的数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METTS MARK\n",
      "BAXTER JOHN C\n",
      "ELLIOTT STEVEN\n",
      "CORDES WILLIAM R\n",
      "HANNON KEVIN P\n",
      "MORDAUNT KRISTINA M\n",
      "MEYER ROCKFORD G\n",
      "MCMAHON JEFFREY\n",
      "HORTON STANLEY C\n",
      "PIPER GREGORY F\n",
      "HUMPHREY GENE E\n",
      "UMANOFF ADAM S\n",
      "BLACHMAN JEREMY M\n",
      "SUNDE MARTIN\n",
      "GIBBS DANA R\n",
      "LOWRY CHARLES P\n",
      "COLWELL WESLEY\n",
      "MULLER MARK S\n",
      "JACKSON CHARLENE R\n",
      "WESTFAHL RICHARD K\n",
      "WALTERS GARETH W\n",
      "WALLS JR ROBERT H\n",
      "KITCHEN LOUISE\n",
      "CHAN RONNIE\n",
      "BELFER ROBERT\n",
      "SHANKMAN JEFFREY A\n",
      "WODRASKA JOHN\n",
      "BERGSIEKER RICHARD P\n",
      "URQUHART JOHN A\n",
      "BIBI PHILIPPE A\n",
      "RIEKER PAULA H\n",
      "WHALEY DAVID A\n",
      "BECK SALLY W\n",
      "HAUG DAVID L\n",
      "ECHOLS JOHN B\n",
      "MENDELSOHN JOHN\n",
      "HICKERSON GARY J\n",
      "CLINE KENNETH W\n",
      "LEWIS RICHARD\n",
      "HAYES ROBERT E\n",
      "MCCARTY DANNY J\n",
      "KOPPER MICHAEL J\n",
      "LEFF DANIEL P\n",
      "LAVORATO JOHN J\n",
      "BERBERIAN DAVID\n",
      "DETMERING TIMOTHY J\n",
      "WAKEHAM JOHN\n",
      "POWERS WILLIAM\n",
      "GOLD JOSEPH\n",
      "BANNANTINE JAMES M\n",
      "DUNCAN JOHN H\n",
      "SHAPIRO RICHARD S\n",
      "SHERRIFF JOHN R\n",
      "SHELBY REX\n",
      "LEMAISTRE CHARLES\n",
      "DEFFNER JOSEPH M\n",
      "KISHKILL JOSEPH G\n",
      "WHALLEY LAWRENCE G\n",
      "MCCONNELL MICHAEL S\n",
      "PIRO JIM\n",
      "DELAINEY DAVID W\n",
      "SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "WROBEL BRUCE\n",
      "LINDHOLM TOD A\n",
      "MEYER JEROME J\n",
      "LAY KENNETH L\n",
      "BUTTS ROBERT H\n",
      "OLSON CINDY K\n",
      "MCDONALD REBECCA\n",
      "CUMBERLAND MICHAEL S\n",
      "GAHN ROBERT S\n",
      "MCCLELLAN GEORGE\n",
      "HERMANN ROBERT J\n",
      "SCRIMSHAW MATTHEW\n",
      "GATHMANN WILLIAM D\n",
      "HAEDICKE MARK E\n",
      "BOWEN JR RAYMOND M\n",
      "GILLIS JOHN\n",
      "FITZGERALD JAY L\n",
      "MORAN MICHAEL P\n",
      "REDMOND BRIAN L\n",
      "BAZELIDES PHILIP J\n",
      "BELDEN TIMOTHY N\n",
      "DURAN WILLIAM D\n",
      "THORN TERENCE H\n",
      "FASTOW ANDREW S\n",
      "FOY JOE\n",
      "CALGER CHRISTOPHER F\n",
      "RICE KENNETH D\n",
      "KAMINSKI WINCENTY J\n",
      "LOCKHART EUGENE E\n",
      "COX DAVID\n",
      "OVERDYKE JR JERE C\n",
      "PEREIRA PAULO V. FERRAZ\n",
      "STABLER FRANK\n",
      "SKILLING JEFFREY K\n",
      "BLAKE JR. NORMAN P\n",
      "SHERRICK JEFFREY B\n",
      "PRENTICE JAMES\n",
      "GRAY RODNEY\n",
      "PICKERING MARK R\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "NOLES JAMES L\n",
      "KEAN STEVEN J\n",
      "TOTAL\n",
      "FOWLER PEGGY\n",
      "WASAFF GEORGE\n",
      "WHITE JR THOMAS E\n",
      "CHRISTODOULOU DIOMEDES\n",
      "ALLEN PHILLIP K\n",
      "SHARP VICTORIA T\n",
      "JAEDICKE ROBERT\n",
      "WINOKUR JR. HERBERT S\n",
      "BROWN MICHAEL\n",
      "BADUM JAMES P\n",
      "HUGHES JAMES A\n",
      "REYNOLDS LAWRENCE\n",
      "DIMICHELE RICHARD G\n",
      "BHATNAGAR SANJAY\n",
      "CARTER REBECCA C\n",
      "BUCHANAN HAROLD G\n",
      "YEAP SOON\n",
      "MURRAY JULIA H\n",
      "GARLAND C KEVIN\n",
      "DODSON KEITH\n",
      "YEAGER F SCOTT\n",
      "HIRKO JOSEPH\n",
      "DIETRICH JANET R\n",
      "DERRICK JR. JAMES V\n",
      "FREVERT MARK A\n",
      "PAI LOU L\n",
      "BAY FRANKLIN R\n",
      "HAYSLETT RODERICK J\n",
      "FUGH JOHN L\n",
      "FALLON JAMES B\n",
      "KOENIG MARK E\n",
      "SAVAGE FRANK\n",
      "IZZO LAWRENCE L\n",
      "TILNEY ELIZABETH A\n",
      "MARTIN AMANDA K\n",
      "BUY RICHARD B\n",
      "GRAMM WENDY L\n",
      "CAUSEY RICHARD A\n",
      "TAYLOR MITCHELL S\n",
      "DONAHUE JR JEFFREY M\n",
      "GLISAN JR BEN F\n"
     ]
    }
   ],
   "source": [
    "for element in my_dataset:\n",
    "    print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del my_dataset['TOTAL']\n",
    "del my_dataset['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除全部为NaN的数据点\n",
    "其次，对数据全部为NaN的点进行清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E\n"
     ]
    }
   ],
   "source": [
    "for element in my_dataset:\n",
    "    for feature in my_dataset[element]:\n",
    "        item = my_dataset[element][feature]\n",
    "        if item == 'NaN' or item == 0:\n",
    "            NaN = True\n",
    "        else:\n",
    "            NaN = False\n",
    "            break\n",
    "    if NaN:\n",
    "        print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "del my_dataset['LOCKHART EUGENE E']\n",
    "print len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据初步探查\n",
    "接下来我尝试使用pandas对数据集进行初步的研究。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "## 引入pandas模块，并创建DataFrame开始分析数据\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "len(df) # 数据集长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     125\n",
       "1      49\n",
       "2      62\n",
       "3      57\n",
       "4     105\n",
       "5      20\n",
       "6      42\n",
       "7      34\n",
       "8      57\n",
       "9     126\n",
       "10     18\n",
       "11     49\n",
       "12    140\n",
       "13    127\n",
       "14     95\n",
       "15     78\n",
       "16     69\n",
       "17     77\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df == 0 ).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现143条记录，其中有近5个字段有超过三分之二的数据是零。在选择特征数据的时候应该加以考量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.125874</td>\n",
       "      <td>1.867429e+05</td>\n",
       "      <td>6.807246e+05</td>\n",
       "      <td>1247.216783</td>\n",
       "      <td>2.236426e+05</td>\n",
       "      <td>2.272323e+06</td>\n",
       "      <td>2.090318e+06</td>\n",
       "      <td>8.746100e+05</td>\n",
       "      <td>707.524476</td>\n",
       "      <td>7.393131e+04</td>\n",
       "      <td>2.930134e+06</td>\n",
       "      <td>35622.720280</td>\n",
       "      <td>5.868881e+05</td>\n",
       "      <td>10050.111888</td>\n",
       "      <td>-1.950377e+05</td>\n",
       "      <td>3.393142e+05</td>\n",
       "      <td>39.027972</td>\n",
       "      <td>24.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.332873</td>\n",
       "      <td>1.971171e+05</td>\n",
       "      <td>1.236180e+06</td>\n",
       "      <td>2243.006069</td>\n",
       "      <td>7.565208e+05</td>\n",
       "      <td>8.876252e+06</td>\n",
       "      <td>4.809193e+06</td>\n",
       "      <td>2.022338e+06</td>\n",
       "      <td>1079.457016</td>\n",
       "      <td>1.306545e+06</td>\n",
       "      <td>6.205937e+06</td>\n",
       "      <td>45370.869604</td>\n",
       "      <td>6.818177e+06</td>\n",
       "      <td>31399.349067</td>\n",
       "      <td>6.079225e+05</td>\n",
       "      <td>6.890139e+05</td>\n",
       "      <td>74.466359</td>\n",
       "      <td>80.031821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.787380e+06</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.504386e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.679650e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.827650e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.549360e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750600e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.106920e+05</td>\n",
       "      <td>3.000000e+05</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.665220e+05</td>\n",
       "      <td>6.087500e+05</td>\n",
       "      <td>3.605280e+05</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.760370e+05</td>\n",
       "      <td>21530.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702590e+05</td>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>1639.000000</td>\n",
       "      <td>9.110000e+03</td>\n",
       "      <td>1.956978e+06</td>\n",
       "      <td>1.698900e+06</td>\n",
       "      <td>7.759920e+05</td>\n",
       "      <td>967.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.307584e+06</td>\n",
       "      <td>53534.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.748255e+05</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.111258e+06</td>\n",
       "      <td>8.000000e+06</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>6.426990e+06</td>\n",
       "      <td>1.035598e+08</td>\n",
       "      <td>3.434838e+07</td>\n",
       "      <td>1.476169e+07</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>1.545629e+07</td>\n",
       "      <td>4.911008e+07</td>\n",
       "      <td>228763.000000</td>\n",
       "      <td>8.152500e+07</td>\n",
       "      <td>137864.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.145434e+06</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4   \\\n",
       "count  143.000000  1.430000e+02  1.430000e+02    143.000000  1.430000e+02   \n",
       "mean     0.125874  1.867429e+05  6.807246e+05   1247.216783  2.236426e+05   \n",
       "std      0.332873  1.971171e+05  1.236180e+06   2243.006069  7.565208e+05   \n",
       "min      0.000000  0.000000e+00  0.000000e+00      0.000000 -1.025000e+05   \n",
       "25%      0.000000  0.000000e+00  0.000000e+00      0.000000  0.000000e+00   \n",
       "50%      0.000000  2.106920e+05  3.000000e+05    383.000000  0.000000e+00   \n",
       "75%      0.000000  2.702590e+05  8.000000e+05   1639.000000  9.110000e+03   \n",
       "max      1.000000  1.111258e+06  8.000000e+06  15149.000000  6.426990e+06   \n",
       "\n",
       "                 5             6             7            8             9   \\\n",
       "count  1.430000e+02  1.430000e+02  1.430000e+02   143.000000  1.430000e+02   \n",
       "mean   2.272323e+06  2.090318e+06  8.746100e+05   707.524476  7.393131e+04   \n",
       "std    8.876252e+06  4.809193e+06  2.022338e+06  1079.457016  1.306545e+06   \n",
       "min    0.000000e+00  0.000000e+00 -2.604490e+06     0.000000 -1.787380e+06   \n",
       "25%    9.679650e+04  0.000000e+00  3.827650e+04     0.000000  0.000000e+00   \n",
       "50%    9.665220e+05  6.087500e+05  3.605280e+05   114.000000  0.000000e+00   \n",
       "75%    1.956978e+06  1.698900e+06  7.759920e+05   967.500000  0.000000e+00   \n",
       "max    1.035598e+08  3.434838e+07  1.476169e+07  5521.000000  1.545629e+07   \n",
       "\n",
       "                 10             11            12             13            14  \\\n",
       "count  1.430000e+02     143.000000  1.430000e+02     143.000000  1.430000e+02   \n",
       "mean   2.930134e+06   35622.720280  5.868881e+05   10050.111888 -1.950377e+05   \n",
       "std    6.205937e+06   45370.869604  6.818177e+06   31399.349067  6.079225e+05   \n",
       "min   -4.409300e+04       0.000000  0.000000e+00       0.000000 -3.504386e+06   \n",
       "25%    2.549360e+05       0.000000  0.000000e+00       0.000000 -3.750600e+04   \n",
       "50%    9.760370e+05   21530.000000  0.000000e+00       0.000000  0.000000e+00   \n",
       "75%    2.307584e+06   53534.500000  0.000000e+00       0.000000  0.000000e+00   \n",
       "max    4.911008e+07  228763.000000  8.152500e+07  137864.000000  0.000000e+00   \n",
       "\n",
       "                 15          16          17  \n",
       "count  1.430000e+02  143.000000  143.000000  \n",
       "mean   3.393142e+05   39.027972   24.797203  \n",
       "std    6.890139e+05   74.466359   80.031821  \n",
       "min    0.000000e+00    0.000000    0.000000  \n",
       "25%    0.000000e+00    0.000000    0.000000  \n",
       "50%    0.000000e+00    4.000000    0.000000  \n",
       "75%    3.748255e+05   41.500000   14.000000  \n",
       "max    5.145434e+06  528.000000  609.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0.0    125\n",
       "1.0     18\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([0])[0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嫌疑人人数18人，非嫌疑人人数125人。分布不平衡，后期评估算法性能的时候应采用precision和recall更加合适。\n",
    "光从这些数据的分布，无法研究出数据与嫌疑人身份的关联性，所以我将开始使用机器学习的方法，来对这些数据进行分析。\n",
    "## 特征工程准备\n",
    "我计划在feature list中覆盖尽可能多的特征，然后使用selectPercentile来进行特征筛选。所以采用的feature list如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'director_fees',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi']\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征缩放\n",
    "使用MinMaxScaler对特征进行缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特征缩放函数\n",
    "def feature_scale(features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征选择\n",
    "我使用了selectPercentile，并且将函数中的percentile参数化，方便日后根据性能进行调整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特征选择函数\n",
    "def feature_selection(features,labels,selector_percentile_parameter):\n",
    "    selector = SelectPercentile(f_classif, percentile = selector_percentile_parameter)\n",
    "    features = selector.fit_transform(features,labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成因分析\n",
    "选用了PCA函数，进行主成因分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 主成因分析\n",
    "def feature_PCA(features,labels,components_parameter):\n",
    "    pca = PCA(n_components=components_parameter)\n",
    "    features = pca.fit_transform(features,labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据转化\n",
    "最后，我构造了一个统一的数据转化函数，可以根据参数对数据调用上述函数对数据特征进行转化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据转化函数\n",
    "def features_transform(features,labels):\n",
    "    features = feature_scale(features)\n",
    "    features = feature_selection(features,labels,selector_percentile_parameter)\n",
    "    features = feature_PCA(features,labels,components_parameter)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述是特征工程所需的功能。\n",
    "## 机器学习算法准备\n",
    "本文尝试使用Naive Bayes，Decision Tree，SVM以及Random Forest四种算法，对数据进行分析，首先，我搭建了一个统一的算法函数，可以通过设置参数选择对应的算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearch_test用以控制是否通过GridSearch寻找合适的参数，函数中的各个算法参数都是经过试验获得的，后文会详细展示试验过程。\n",
    "# 通过输入不同的函数名选用不同的参数 \"NB\",\"Decision Tree\",\"Random Forest\",\"SVM\"\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def classifier(algorithm = \"Decision Tree\", GridSearch_test = False):\n",
    "    if algorithm == 'NB':\n",
    "        ## GaussianNB\n",
    "        clf = GaussianNB()\n",
    "    elif algorithm == 'Decision Tree':\n",
    "        ## Decision Tree\n",
    "        clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 2,min_samples_leaf = 9)  \n",
    "        if GridSearch_test:\n",
    "            parameters = {'criterion':[\"entropy\",\"gini\"],'max_depth':(1,10,1),'min_samples_leaf':(1,200,10)}\n",
    "            clf = GridSearchCV(clf,parameters)\n",
    "    elif algorithm == 'Random Forest':\n",
    "        ## Random Forest\n",
    "        clf = RandomForestClassifier(n_estimators = 3)\n",
    "        if GridSearch_test:\n",
    "            parameters = {'n_estimators':[1,10]}\n",
    "            clf = GridSearchCV(clf,parameters)\n",
    "    elif algorithm == 'SVM':\n",
    "        clf = SVC(C=1,gamma = 1)\n",
    "        if GridSearch_test:\n",
    "            parameters = {'C':[0.001, 0.01, 0.1, 1, 10],\n",
    "            \"gamma\":[0.001, 0.01, 0.1, 1]}\n",
    "            clf = GridSearchCV(clf, parameters)\n",
    "    return clf\n",
    "\n",
    "clf = classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法评估准备\n",
    "首先我定义了评估的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当print_score 为 Ture时，打印算法的accuracy，precision和recall\n",
    "def calculate_score(clf,features,labels,print_score = False):\n",
    "    ## import precision score evaluation\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    accuracy = accuracy_score(clf.predict(features),labels)\n",
    "    precision = precision_score(clf.predict(features),labels)\n",
    "    recall = recall_score(clf.predict(features),labels)\n",
    "    \n",
    "    if print_score:\n",
    "        print clf\n",
    "        print \"accuracy score: {}\".format(accuracy)\n",
    "        print \"precision score: {}\".format(precision)\n",
    "        print \"recall score: {}\".format(recall)\n",
    "    \n",
    "    scores = {\"accuracy\":accuracy,\n",
    "           \"precision\":precision,\n",
    "           \"recall\":recall\n",
    "            }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初我选用了简单的train_test_split，进行算法性能评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy score: 0.844827586207\n",
      "precision score: 0.428571428571\n",
      "recall score: 0.375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.84482758620689657,\n",
       " 'precision': 0.42857142857142855,\n",
       " 'recall': 0.375}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "clf = classifier()\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "calculate_score(clf,features_test,labels_test,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "后来我发现这样测算的准确率与最后tester中运行的检测结果相差很多。所以我改用kfold，取score平局值进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "average accuracy:0.868571428571\n",
      "precision accuracy:0.183333333333\n",
      "recall accuracy:0.3\n"
     ]
    }
   ],
   "source": [
    "# 搭建kfold测试数据组\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(10)\n",
    "cv = kf.split(features)\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for train_index,test_index in cv:\n",
    "    features_train = [features[ii] for ii in train_index]\n",
    "    features_test = [features[ii] for ii in test_index]\n",
    "    labels_train = [labels[ii] for ii in train_index]\n",
    "    labels_test = [labels[ii] for ii in test_index]\n",
    "    clf.fit(features_train,labels_train)\n",
    "    score = calculate_score(clf,features_test,labels_test) # 计算每个数据组的数据情况，并且添加到对应list中\n",
    "    accuracy_list.append(score['accuracy'])\n",
    "    precision_list.append(score['precision'])\n",
    "    recall_list.append(score['recall'])\n",
    "# 计算各个list的均值\n",
    "print clf\n",
    "print \"average accuracy:{}\".format(sum(accuracy_list)/len(accuracy_list))\n",
    "print \"precision accuracy:{}\".format(sum(precision_list)/len(precision_list))\n",
    "print \"recall accuracy:{}\".format(sum(recall_list)/len(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后我发现这样的评估效果是会好一些，但是还是不及tester中的评估效果。所以我最后复用了tester中的语句进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.87160\tPrecision: 0.52812\tRecall: 0.34750\tF1: 0.41918\tF2: 0.37301\n",
      "\tTotal predictions: 15000\tTrue positives:  695\tFalse positives:  621\tFalse negatives: 1305\tTrue negatives: 12379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "folds = 1000\n",
    "\n",
    "# \n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list,transform = True):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        ### feature engineer the features\n",
    "        if transform:\n",
    "            features_train = features_transform(features_train,labels_train)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        ### transform test features\n",
    "        if transform:\n",
    "            features_test = features_transform(features_test,labels_test)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "\n",
    "    try:\n",
    "        \n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数调整\n",
    "首先，我使用GridSearchCV，对各种算法的参数进行了调整：\n",
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = classifier(\"Decision Tree\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.85800\tPrecision: 0.00758\tRecall: 0.00050\tF1: 0.00094\tF2: 0.00061\n",
      "\tTotal predictions: 15000\tTrue positives:    1\tFalse positives:  131\tFalse negatives: 1999\tTrue negatives: 12869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 1,min_samples_leaf = 2)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf = classifier(\"Random Forest\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.84960\tPrecision: 0.30368\tRecall: 0.09900\tF1: 0.14932\tF2: 0.11442\n",
      "\tTotal predictions: 15000\tTrue positives:  198\tFalse positives:  454\tFalse negatives: 1802\tTrue negatives: 12546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 2)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "clf = classifier(\"SVM\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=0.001,gamma=0.001)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述的几次测试来看GridSearch都没有达到比较好的效果。所以我最后采用了手动调整的方法，手动调整各个算法中的参数，以及选用特征的比例（feature_selection：selector_percentile），和构造新特征的个数（PCA：components）。\n",
    "### 手动调整\n",
    "手动调整算法参数以及特征参数的结果如下：\n",
    "#### Decision Tree\n",
    "algorithm\tKenel\tcomponents\tselector_percentile\tresult\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\tNone\tNone\tAccuracy: 0.87213       Precision: 0.53135      Recall: 0.34750 F1: 0.42019     F2: 0.37333\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\t2\t15\tAccuracy: 0.83807       Precision: 0.31960      Recall: 0.19000 F1: 0.23832     F2: 0.20677\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\t2\t20\tAccuracy: 0.84773       Precision: 0.39605      Recall: 0.27050 F1: 0.32145     F2: 0.28881\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\t2\t30\tAccuracy: 0.84193       Precision: 0.40283      Recall: 0.38450 F1: 0.39345     F2: 0.38803\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\t2\t35\tAccuracy: 0.83027       Precision: 0.36773      Recall: 0.37950 F1: 0.37352     F2: 0.37709\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"entropy\",max_depth = 2,min_samples_leaf = 9\t3\t35\tAccuracy: 0.83040       Precision: 0.36640      Recall: 0.37300 F1: 0.36967     F2: 0.37166\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\tNone\tNone\tAccuracy: 0.85673       Precision: 0.42375      Recall: 0.20700 F1: 0.27813     F2: 0.23059\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\t2\t50\tAccuracy: 0.83053       Precision: 0.29563      Recall: 0.19600 F1: 0.23572     F2: 0.21017\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\t2\t30\tAccuracy: 0.83753       Precision: 0.39590      Recall: 0.41550 F1: 0.40546     F2: 0.41143\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\t2\t35\tAccuracy: 0.82820       Precision: 0.36411      Recall: 0.38650 F1: 0.37497     F2: 0.38180\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\t2\t20\tAccuracy: 0.84167       Precision: 0.38404      Recall: 0.31050 F1: 0.34338     F2: 0.32287\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 9\t3\t30\tAccuracy: 0.84327       Precision: 0.39147      Recall: 0.31650 F1: 0.35001     F2: 0.32910\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 3,min_samples_leaf = 10\t3\t30\tAccuracy: 0.83147       Precision: 0.35085      Recall: 0.31050 F1: 0.32944     F2: 0.31781\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 3,min_samples_leaf = 5\t3\t30\tAccuracy: 0.83600       Precision: 0.37920      Recall: 0.36100 F1: 0.36988     F2: 0.36450\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 2,min_samples_leaf = 3\t3\t30\tAccuracy: 0.84953 Precision: 0.40031 Recall: 0.25800 F1: 0.31377 F2: 0.27775\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 1,min_samples_leaf = 3\t3\t30\tAccuracy: 0.85747       Precision: 0.43658      Recall: 0.23750 F1: 0.30764     F2: 0.26133\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 3,min_samples_leaf = 3\t3\t30\tAccuracy: 0.83133       Precision: 0.36269      Recall: 0.35000 F1: 0.35623     F2: 0.35247\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 3,min_samples_leaf = 3\t2\t30\tAccuracy: 0.83907       Precision: 0.38774      Recall: 0.35750 F1: 0.37201     F2: 0.36317\t\t\t\t\t\t\t\t\t\t\t\n",
    "Decision Tree\tcriterion = \"gini\",max_depth = 3,min_samples_leaf = 9\t2\t30\tAccuracy: 0.83367 Precision: 0.38896 Recall: 0.43350 F1: 0.41003 F2: 0.42380\t\t\t\t\n",
    "\n",
    "最终得出参数criterion = \"entropy\",max_depth = 2,min_samples_leaf = 9 无需feature engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.87180\tPrecision: 0.52932\tRecall: 0.34750\tF1: 0.41956\tF2: 0.37313\n",
      "\tTotal predictions: 15000\tTrue positives:  695\tFalse positives:  618\tFalse negatives: 1305\tTrue negatives: 12382\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 2,min_samples_leaf = 9)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "algorithm\tKenel\tcomponents\tselector_percentile\tresult\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\tNone\tNone\tAccuracy: 0.41907       Precision: 0.17274      Recall: 0.88600 F1: 0.28912     F2: 0.48527\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t1\t7\tAccuracy: 0.78787 Precision: 0.35422 Recall: 0.71800 F1: 0.47440 F2: 0.59565\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t1\t20\tAccuracy: 0.79787       Precision: 0.36576      Recall: 0.70300 F1: 0.48118     F2: 0.59355\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t1\t40\tAccuracy: 0.82560       Precision: 0.39761      Recall: 0.59800 F1: 0.47764     F2: 0.54324\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t2\t40\tAccuracy: 0.73493       Precision: 0.30582      Recall: 0.77800 F1: 0.43905     F2: 0.59444\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t2\t50\tAccuracy: 0.72813       Precision: 0.29996      Recall: 0.77900 F1: 0.43314     F2: 0.59042\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t3\t50\tAccuracy: 0.68320       Precision: 0.27097      Recall: 0.81400 F1: 0.40659     F2: 0.58110\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t5\t50\tAccuracy: 0.65707       Precision: 0.25696      Recall: 0.83100 F1: 0.39254     F2: 0.57437\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t2\t30\tAccuracy: 0.78520       Precision: 0.35954      Recall: 0.78200 F1: 0.49260     F2: 0.63320\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t2\t20\tAccuracy: 0.76640       Precision: 0.33723      Recall: 0.77900 F1: 0.47069     F2: 0.61727\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t2\t15\tAccuracy: 0.61747       Precision: 0.23414      Recall: 0.82300 F1: 0.36456     F2: 0.54757\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t1\t5\tAccuracy: 0.78787       Precision: 0.35422      Recall: 0.71800 F1: 0.47440     F2: 0.59565\t\t\t\t\t\t\t\t\t\t\t\n",
    "Naive Bayes\tdefault\t1\t35\tAccuracy: 0.84280       Precision: 0.43819      Recall: 0.63450 F1: 0.51838     F2: 0.58232\t\t\t\t\t\t\t\t\t\t\t\n",
    "  \n",
    "最终得出参数component:1, selector_percentile:35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.84280\tPrecision: 0.43819\tRecall: 0.63450\tF1: 0.51838\tF2: 0.58232\n",
      "\tTotal predictions: 15000\tTrue positives: 1269\tFalse positives: 1627\tFalse negatives:  731\tTrue negatives: 11373\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector_percentile_parameter = 35\n",
    "components_parameter = 1\n",
    "clf = clf = GaussianNB()\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "algorithm\tKenel\tcomponents\tselector_percentile\tresult\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\tNone\tNone\tAccuracy: 0.83513       Precision: 0.33167      Recall: 0.23300 F1: 0.27372     F2: 0.24774\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 4\tNone\tNone\tAccuracy: 0.85640       Precision: 0.39895      Recall: 0.15200 F1: 0.22013     F2: 0.17348\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 2\tNone\tNone\tAccuracy: 0.85107       Precision: 0.31429      Recall: 0.09900 F1: 0.15057     F2: 0.11472\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t2\t30\tAccuracy: 0.81733       Precision: 0.34910      Recall: 0.42800 F1: 0.38455     F2: 0.40949\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t2\t50\tAccuracy: 0.78453       Precision: 0.28611      Recall: 0.41200 F1: 0.33770     F2: 0.37868\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t2\t\tAccuracy: 0.78693       Precision: 0.28427      Recall: 0.39400 F1: 0.33026     F2: 0.36576\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t2\t20\tAccuracy: 0.82800       Precision: 0.38725      Recall: 0.49800 F1: 0.43570     F2: 0.47106\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t2\t10\tAccuracy: 0.78373       Precision: 0.31749      Recall: 0.54100 F1: 0.40015     F2: 0.47423\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t3\t20\tAccuracy: 0.81200       Precision: 0.34182      Recall: 0.44300 F1: 0.38589     F2: 0.41824\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t1\t20\tAccuracy: 0.87387       Precision: 0.52373      Recall: 0.59600 F1: 0.55753     F2: 0.57999\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t1\t10\tAccuracy: 0.87987       Precision: 0.54925      Recall: 0.55200 F1: 0.55062     F2: 0.55145\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t1\t5\tAccuracy: 0.88547       Precision: 0.57860      Recall: 0.51900 F1: 0.54718     F2: 0.52992\t\t\t\t\t\t\t\t\t\t\t\n",
    "Random Forest\tn_estimators = 3\t1\t7\tAccuracy: 0.89107       Precision: 0.59150      Recall: 0.59150 F1: 0.59150     F2: 0.59150\t\t\t\t\t\t\t\t\n",
    "最终得出参数component:1, selector_percentile:7 n_estimators = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=3, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.89920\tPrecision: 0.63895\tRecall: 0.56100\tF1: 0.59744\tF2: 0.57503\n",
      "\tTotal predictions: 15000\tTrue positives: 1122\tFalse positives:  634\tFalse negatives:  878\tTrue negatives: 12366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 1\n",
    "selector_percentile_parameter = 5\n",
    "clf = RandomForestClassifier(n_estimators = 3)\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "algorithm\tKenel\tcomponents\tselector_percentile\tresult\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\tNone\tNone\tPrecision or recall may be undefined due to a lack of true positive predicitons.\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t1\t7\tAccuracy: 0.90153       Precision: 0.83143      Recall: 0.32800 F1: 0.47042     F2: 0.37319\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t1\t20\tAccuracy: 0.90727       Precision: 0.76687      Recall: 0.43750 F1: 0.55715     F2: 0.47861\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t1\t30\tAccuracy: 0.90067       Precision: 0.77070      Recall: 0.36300 F1: 0.49354     F2: 0.40595\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t2\t30\tAccuracy: 0.89340       Precision: 0.78279      Recall: 0.27750 F1: 0.40975     F2: 0.31864\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t1\t40\tAccuracy: 0.88047       Precision: 0.62191      Recall: 0.26400 F1: 0.37066     F2: 0.29834\n",
    "Total predictions: 15000        True positives:  528    False positives:  321   False negatives: 1472   True negatives: 12679\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tdefault\t1\t15\tAccuracy: 0.90493       Precision: 0.80728      Recall: 0.37700 F1: 0.51397     F2: 0.42198\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=1\t1\t15\tAccuracy: 0.90493 Precision: 0.80728 Recall: 0.37700 F1: 0.51397 F2: 0.42198\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=10\t1\t15\tAccuracy: 0.90493 Precision: 0.80728 Recall: 0.37700 F1: 0.51397 F2: 0.42198\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=10\t1\t15\tAccuracy: 0.90740       Precision: 0.69748      Recall: 0.53950 F1: 0.60840     F2: 0.56510\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=5\t1\t15\tAccuracy: 0.91180       Precision: 0.75939      Recall: 0.49550 F1: 0.59970     F2: 0.53251\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=5,gama=5\t1\t15\tAccuracy: 0.91180       Precision: 0.75939      Recall: 0.49550 F1: 0.59970     F2: 0.53251\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=2,gama=1\t1\t15\tAccuracy: 0.91207       Precision: 0.80213      Recall: 0.45200 F1: 0.57819     F2: 0.49523\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=1,gama=1\t1\t15\tAccuracy: 0.90493       Precision: 0.80728      Recall: 0.37700 F1: 0.51397     F2: 0.42198\t\t\t\t\t\t\t\t\t\t\t\n",
    "SVM\tC=1,gama=1\t1\t10\tAccuracy: 0.90927       Precision: 0.84615      Recall: 0.39050 F1: 0.53438     F2: 0.43763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.91207\tPrecision: 0.80213\tRecall: 0.45200\tF1: 0.57819\tF2: 0.49523\n",
      "\tTotal predictions: 15000\tTrue positives:  904\tFalse positives:  223\tFalse negatives: 1096\tTrue negatives: 12777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 1\n",
    "selector_percentile_parameter = 15\n",
    "clf = SVC(C=1,gamma = 1)\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后整体调整参数可得SVM这个算法是最优的算法，Accuracy: 0.91207\tPrecision: 0.80213\tRecall: 0.45200\tF1: 0.57819\tF2: 0.49523。  \n",
    "其参数为：  \n",
    "- components_parameter = 1  \n",
    "- selector_percentile_parameter = 15  \n",
    "- C=1  \n",
    "- gamma = 1  \n",
    "\n",
    "## 问题总结\n",
    "\n",
    "- 你在获得数据时它们是否包含任何异常值，你是如何进行处理的？【相关标准项：“数据探索”，“异常值调查”】\n",
    "\n",
    "包含了部分异常值，一部分异常值是数据统计时产生的异常值，如Total的值，直接删除。  \n",
    "还有一部分是NaN的值，通过FeatureFormat函数进行了转化，把NaN的函数变为了0值。  \n",
    "\n",
    "- 你最终在你的 POI 标识符中使用了什么特征，你使用了什么筛选过程来挑选它们？你是否需要进行任何缩放？为什么？作为任务的一部分，你应该尝试设计自己的特征，而非使用数据集中现成的——解释你尝试创建的特征及其基本原理。（你不一定要在最后的分析中使用它，而只设计并测试它）。在你的特征选择步骤，如果你使用了算法（如决策树），请也给出所使用特征的特征重要性；如果你使用了自动特征选择函数（如 SelectBest），请报告特征得分及你所选的参数值的原因。【相关标准项：“创建新特征”、“适当缩放特征”、“智能选择功能”】\n",
    "\n",
    "首先我添加了尽可能多的参数，可以保证在算法训练中使用到尽可能多的信息。接下来我通过SelectPercentile来选择参数，这样可以方便调节选用哪些参数，提升或减少用以训练的信息量。我使用了PCA创建了新的特征，并且用MinMax进行了特征缩放，根据调参的结果，这样做可以提升算法性能。  \n",
    "\n",
    "- 你最终使用了什么算法？你还尝试了其他什么算法？不同算法之间的模型性能有何差异？【相关标准项：“选择算法”】  \n",
    "\n",
    "我最终使用了SVM，我还尝试了Naive Bayes、Random Forest以及Decision Tree。Naive Bayes召回率最高，但是精准度难以提升。Random Forest通过调整可以有很高的正确率和召回率，但是相对耗时偏长。Decision Tree在使用Entrophy时，无需调整特征就可以达到很高的精准度和召回率。但是总和评判下来，SVM拥有最高的分类准确度、正确率、召回率，而且预测时间也可以接受。  \n",
    "\n",
    "- 调整算法的参数是什么意思，如果你不这样做会发生什么？你是如何调整特定算法的参数的？（一些算法没有需要调整的参数 – 如果你选择的算法是这种情况，指明并简要解释对于你最终未选择的模型或需要参数调整的不同模型，例如决策树分类器，你会怎么做）。【相关标准项：“调整算法”】 \n",
    "  \n",
    "我使用了GridSearcCV进行调参，但是我发现会出现过度拟合的情况。可能的问题是样本量设置过大，导致过度拟合。最后我是通过手动调参，并且记录的方式进行调整，这样做可以结合precision和recall的情况对参数做目标性的调整，效率可以更加高。  \n",
    "\n",
    "- 什么是验证，未正确执行情况下的典型错误是什么？你是如何验证你的分析的？【相关标准项：“验证策略”】  \n",
    "\n",
    "典型错误就是过度拟合，在样本中表现非常好，但是在测试过程中表现就不太好。遇到这样的问题，我会适当减少训练算法的深度，减少拟合的水平。  \n",
    "\n",
    "- 给出至少 2 个评估度量并说明每个的平均性能。解释对用简单的语言表明算法性能的度量的解读。【相关标准项：“评估度量的使用”】\n",
    "\n",
    "precision rate：预测为嫌疑人中真正是嫌疑人的占比。即测的对不对。  \n",
    "recall rate：所有嫌疑人被识别出的人的占比。即测的全不全。  \n",
    "\n",
    "以上是对数据集的分析。其实还有很多问题没有解决：  \n",
    "\n",
    "- 如何通过机器的方法自动调整包括特征工程在内的参数  \n",
    "- 是否有其他算法，比文中所选的算法更加有效  \n",
    "- 是否在数据点足够多的情况下，是否要将时间也考虑在内  \n",
    "\n",
    "## 关于代码\n",
    "本文中覆盖了所有与结果相关的代码。随文附上调参时使用的代码，参数均已设置为最优算法与参数。为了能够体现特征工程的效果，我在tester文件中添加了特征工程的相关功能，供测试使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ana41py27]",
   "language": "python",
   "name": "conda-env-ana41py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
