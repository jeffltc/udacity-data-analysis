{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习项目作业\n",
    "本文尝试对数据集进行研究，并使用机器学习的方法，挖掘可能存在的嫌疑人。\n",
    "\n",
    "## 数据清洗\n",
    "首先，我引入本次研究所使用的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "from tester import test_classifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    my_dataset = data_dict\n",
    "    features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'to_messages',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'director_fees', \n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi']\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    }
   ],
   "source": [
    "print len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除非人数据点\n",
    "首先，打印数据所有人的名称，去除明显非人的数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METTS MARK\n",
      "BAXTER JOHN C\n",
      "ELLIOTT STEVEN\n",
      "CORDES WILLIAM R\n",
      "HANNON KEVIN P\n",
      "MORDAUNT KRISTINA M\n",
      "MEYER ROCKFORD G\n",
      "MCMAHON JEFFREY\n",
      "HORTON STANLEY C\n",
      "PIPER GREGORY F\n",
      "HUMPHREY GENE E\n",
      "UMANOFF ADAM S\n",
      "BLACHMAN JEREMY M\n",
      "SUNDE MARTIN\n",
      "GIBBS DANA R\n",
      "LOWRY CHARLES P\n",
      "COLWELL WESLEY\n",
      "MULLER MARK S\n",
      "JACKSON CHARLENE R\n",
      "WESTFAHL RICHARD K\n",
      "WALTERS GARETH W\n",
      "WALLS JR ROBERT H\n",
      "KITCHEN LOUISE\n",
      "CHAN RONNIE\n",
      "BELFER ROBERT\n",
      "SHANKMAN JEFFREY A\n",
      "WODRASKA JOHN\n",
      "BERGSIEKER RICHARD P\n",
      "URQUHART JOHN A\n",
      "BIBI PHILIPPE A\n",
      "RIEKER PAULA H\n",
      "WHALEY DAVID A\n",
      "BECK SALLY W\n",
      "HAUG DAVID L\n",
      "ECHOLS JOHN B\n",
      "MENDELSOHN JOHN\n",
      "HICKERSON GARY J\n",
      "CLINE KENNETH W\n",
      "LEWIS RICHARD\n",
      "HAYES ROBERT E\n",
      "MCCARTY DANNY J\n",
      "KOPPER MICHAEL J\n",
      "LEFF DANIEL P\n",
      "LAVORATO JOHN J\n",
      "BERBERIAN DAVID\n",
      "DETMERING TIMOTHY J\n",
      "WAKEHAM JOHN\n",
      "POWERS WILLIAM\n",
      "GOLD JOSEPH\n",
      "BANNANTINE JAMES M\n",
      "DUNCAN JOHN H\n",
      "SHAPIRO RICHARD S\n",
      "SHERRIFF JOHN R\n",
      "SHELBY REX\n",
      "LEMAISTRE CHARLES\n",
      "DEFFNER JOSEPH M\n",
      "KISHKILL JOSEPH G\n",
      "WHALLEY LAWRENCE G\n",
      "MCCONNELL MICHAEL S\n",
      "PIRO JIM\n",
      "DELAINEY DAVID W\n",
      "SULLIVAN-SHAKLOVITZ COLLEEN\n",
      "WROBEL BRUCE\n",
      "LINDHOLM TOD A\n",
      "MEYER JEROME J\n",
      "LAY KENNETH L\n",
      "BUTTS ROBERT H\n",
      "OLSON CINDY K\n",
      "MCDONALD REBECCA\n",
      "CUMBERLAND MICHAEL S\n",
      "GAHN ROBERT S\n",
      "MCCLELLAN GEORGE\n",
      "HERMANN ROBERT J\n",
      "SCRIMSHAW MATTHEW\n",
      "GATHMANN WILLIAM D\n",
      "HAEDICKE MARK E\n",
      "BOWEN JR RAYMOND M\n",
      "GILLIS JOHN\n",
      "FITZGERALD JAY L\n",
      "MORAN MICHAEL P\n",
      "REDMOND BRIAN L\n",
      "BAZELIDES PHILIP J\n",
      "BELDEN TIMOTHY N\n",
      "DURAN WILLIAM D\n",
      "THORN TERENCE H\n",
      "FASTOW ANDREW S\n",
      "FOY JOE\n",
      "CALGER CHRISTOPHER F\n",
      "RICE KENNETH D\n",
      "KAMINSKI WINCENTY J\n",
      "LOCKHART EUGENE E\n",
      "COX DAVID\n",
      "OVERDYKE JR JERE C\n",
      "PEREIRA PAULO V. FERRAZ\n",
      "STABLER FRANK\n",
      "SKILLING JEFFREY K\n",
      "BLAKE JR. NORMAN P\n",
      "SHERRICK JEFFREY B\n",
      "PRENTICE JAMES\n",
      "GRAY RODNEY\n",
      "PICKERING MARK R\n",
      "THE TRAVEL AGENCY IN THE PARK\n",
      "NOLES JAMES L\n",
      "KEAN STEVEN J\n",
      "TOTAL\n",
      "FOWLER PEGGY\n",
      "WASAFF GEORGE\n",
      "WHITE JR THOMAS E\n",
      "CHRISTODOULOU DIOMEDES\n",
      "ALLEN PHILLIP K\n",
      "SHARP VICTORIA T\n",
      "JAEDICKE ROBERT\n",
      "WINOKUR JR. HERBERT S\n",
      "BROWN MICHAEL\n",
      "BADUM JAMES P\n",
      "HUGHES JAMES A\n",
      "REYNOLDS LAWRENCE\n",
      "DIMICHELE RICHARD G\n",
      "BHATNAGAR SANJAY\n",
      "CARTER REBECCA C\n",
      "BUCHANAN HAROLD G\n",
      "YEAP SOON\n",
      "MURRAY JULIA H\n",
      "GARLAND C KEVIN\n",
      "DODSON KEITH\n",
      "YEAGER F SCOTT\n",
      "HIRKO JOSEPH\n",
      "DIETRICH JANET R\n",
      "DERRICK JR. JAMES V\n",
      "FREVERT MARK A\n",
      "PAI LOU L\n",
      "BAY FRANKLIN R\n",
      "HAYSLETT RODERICK J\n",
      "FUGH JOHN L\n",
      "FALLON JAMES B\n",
      "KOENIG MARK E\n",
      "SAVAGE FRANK\n",
      "IZZO LAWRENCE L\n",
      "TILNEY ELIZABETH A\n",
      "MARTIN AMANDA K\n",
      "BUY RICHARD B\n",
      "GRAMM WENDY L\n",
      "CAUSEY RICHARD A\n",
      "TAYLOR MITCHELL S\n",
      "DONAHUE JR JEFFREY M\n",
      "GLISAN JR BEN F\n"
     ]
    }
   ],
   "source": [
    "for element in my_dataset:\n",
    "    print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del my_dataset['TOTAL']\n",
    "del my_dataset['THE TRAVEL AGENCY IN THE PARK']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 去除全部为NaN的数据点\n",
    "其次，对数据全部为NaN的点进行清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCKHART EUGENE E\n"
     ]
    }
   ],
   "source": [
    "for element in my_dataset:\n",
    "    for feature in my_dataset[element]:\n",
    "        item = my_dataset[element][feature]\n",
    "        if item == 'NaN' or item == 0:\n",
    "            NaN = True\n",
    "        else:\n",
    "            NaN = False\n",
    "            break\n",
    "    if NaN:\n",
    "        print element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    }
   ],
   "source": [
    "del my_dataset['LOCKHART EUGENE E']\n",
    "print len(my_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据初步探查\n",
    "接下来我尝试使用pandas对数据集进行初步的研究。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "## 引入pandas模块，并创建DataFrame开始分析数据\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "len(df) # 数据集长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     125\n",
       "1      49\n",
       "2      62\n",
       "3      57\n",
       "4     105\n",
       "5      20\n",
       "6      42\n",
       "7      34\n",
       "8      57\n",
       "9     126\n",
       "10     18\n",
       "11     49\n",
       "12    140\n",
       "13    127\n",
       "14     95\n",
       "15     78\n",
       "16     69\n",
       "17     77\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df == 0 ).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以发现143条记录，其中有近5个字段有超过三分之二的数据是零。在选择特征数据的时候应该加以考量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>1.430000e+02</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.125874</td>\n",
       "      <td>1.867429e+05</td>\n",
       "      <td>6.807246e+05</td>\n",
       "      <td>1247.216783</td>\n",
       "      <td>2.236426e+05</td>\n",
       "      <td>2.272323e+06</td>\n",
       "      <td>2.090318e+06</td>\n",
       "      <td>8.746100e+05</td>\n",
       "      <td>707.524476</td>\n",
       "      <td>7.393131e+04</td>\n",
       "      <td>2.930134e+06</td>\n",
       "      <td>35622.720280</td>\n",
       "      <td>5.868881e+05</td>\n",
       "      <td>10050.111888</td>\n",
       "      <td>-1.950377e+05</td>\n",
       "      <td>3.393142e+05</td>\n",
       "      <td>39.027972</td>\n",
       "      <td>24.797203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.332873</td>\n",
       "      <td>1.971171e+05</td>\n",
       "      <td>1.236180e+06</td>\n",
       "      <td>2243.006069</td>\n",
       "      <td>7.565208e+05</td>\n",
       "      <td>8.876252e+06</td>\n",
       "      <td>4.809193e+06</td>\n",
       "      <td>2.022338e+06</td>\n",
       "      <td>1079.457016</td>\n",
       "      <td>1.306545e+06</td>\n",
       "      <td>6.205937e+06</td>\n",
       "      <td>45370.869604</td>\n",
       "      <td>6.818177e+06</td>\n",
       "      <td>31399.349067</td>\n",
       "      <td>6.079225e+05</td>\n",
       "      <td>6.890139e+05</td>\n",
       "      <td>74.466359</td>\n",
       "      <td>80.031821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.025000e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.604490e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.787380e+06</td>\n",
       "      <td>-4.409300e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.504386e+06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.679650e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.827650e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.549360e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.750600e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.106920e+05</td>\n",
       "      <td>3.000000e+05</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.665220e+05</td>\n",
       "      <td>6.087500e+05</td>\n",
       "      <td>3.605280e+05</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.760370e+05</td>\n",
       "      <td>21530.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.702590e+05</td>\n",
       "      <td>8.000000e+05</td>\n",
       "      <td>1639.000000</td>\n",
       "      <td>9.110000e+03</td>\n",
       "      <td>1.956978e+06</td>\n",
       "      <td>1.698900e+06</td>\n",
       "      <td>7.759920e+05</td>\n",
       "      <td>967.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.307584e+06</td>\n",
       "      <td>53534.500000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.748255e+05</td>\n",
       "      <td>41.500000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.111258e+06</td>\n",
       "      <td>8.000000e+06</td>\n",
       "      <td>15149.000000</td>\n",
       "      <td>6.426990e+06</td>\n",
       "      <td>1.035598e+08</td>\n",
       "      <td>3.434838e+07</td>\n",
       "      <td>1.476169e+07</td>\n",
       "      <td>5521.000000</td>\n",
       "      <td>1.545629e+07</td>\n",
       "      <td>4.911008e+07</td>\n",
       "      <td>228763.000000</td>\n",
       "      <td>8.152500e+07</td>\n",
       "      <td>137864.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.145434e+06</td>\n",
       "      <td>528.000000</td>\n",
       "      <td>609.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4   \\\n",
       "count  143.000000  1.430000e+02  1.430000e+02    143.000000  1.430000e+02   \n",
       "mean     0.125874  1.867429e+05  6.807246e+05   1247.216783  2.236426e+05   \n",
       "std      0.332873  1.971171e+05  1.236180e+06   2243.006069  7.565208e+05   \n",
       "min      0.000000  0.000000e+00  0.000000e+00      0.000000 -1.025000e+05   \n",
       "25%      0.000000  0.000000e+00  0.000000e+00      0.000000  0.000000e+00   \n",
       "50%      0.000000  2.106920e+05  3.000000e+05    383.000000  0.000000e+00   \n",
       "75%      0.000000  2.702590e+05  8.000000e+05   1639.000000  9.110000e+03   \n",
       "max      1.000000  1.111258e+06  8.000000e+06  15149.000000  6.426990e+06   \n",
       "\n",
       "                 5             6             7            8             9   \\\n",
       "count  1.430000e+02  1.430000e+02  1.430000e+02   143.000000  1.430000e+02   \n",
       "mean   2.272323e+06  2.090318e+06  8.746100e+05   707.524476  7.393131e+04   \n",
       "std    8.876252e+06  4.809193e+06  2.022338e+06  1079.457016  1.306545e+06   \n",
       "min    0.000000e+00  0.000000e+00 -2.604490e+06     0.000000 -1.787380e+06   \n",
       "25%    9.679650e+04  0.000000e+00  3.827650e+04     0.000000  0.000000e+00   \n",
       "50%    9.665220e+05  6.087500e+05  3.605280e+05   114.000000  0.000000e+00   \n",
       "75%    1.956978e+06  1.698900e+06  7.759920e+05   967.500000  0.000000e+00   \n",
       "max    1.035598e+08  3.434838e+07  1.476169e+07  5521.000000  1.545629e+07   \n",
       "\n",
       "                 10             11            12             13            14  \\\n",
       "count  1.430000e+02     143.000000  1.430000e+02     143.000000  1.430000e+02   \n",
       "mean   2.930134e+06   35622.720280  5.868881e+05   10050.111888 -1.950377e+05   \n",
       "std    6.205937e+06   45370.869604  6.818177e+06   31399.349067  6.079225e+05   \n",
       "min   -4.409300e+04       0.000000  0.000000e+00       0.000000 -3.504386e+06   \n",
       "25%    2.549360e+05       0.000000  0.000000e+00       0.000000 -3.750600e+04   \n",
       "50%    9.760370e+05   21530.000000  0.000000e+00       0.000000  0.000000e+00   \n",
       "75%    2.307584e+06   53534.500000  0.000000e+00       0.000000  0.000000e+00   \n",
       "max    4.911008e+07  228763.000000  8.152500e+07  137864.000000  0.000000e+00   \n",
       "\n",
       "                 15          16          17  \n",
       "count  1.430000e+02  143.000000  143.000000  \n",
       "mean   3.393142e+05   39.027972   24.797203  \n",
       "std    6.890139e+05   74.466359   80.031821  \n",
       "min    0.000000e+00    0.000000    0.000000  \n",
       "25%    0.000000e+00    0.000000    0.000000  \n",
       "50%    0.000000e+00    4.000000    0.000000  \n",
       "75%    3.748255e+05   41.500000   14.000000  \n",
       "max    5.145434e+06  528.000000  609.000000  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0.0    125\n",
       "1.0     18\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([0])[0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嫌疑人人数18人，非嫌疑人人数125人。分布不平衡，后期评估算法性能的时候应采用precision和recall更加合适。\n",
    "光从这些数据的分布，无法研究出数据与嫌疑人身份的关联性，所以我将开始使用机器学习的方法，来对这些数据进行分析。\n",
    "## 特征工程准备\n",
    "### 新特征\n",
    "首先我根据to_messages,from_messages,from_this_person_to_poi,from_poi_to_this_person四个值，计算与poi相关的邮件数与总体邮件数之比，构造一个名为poi_email_rate新的特征。我认为如果一个人的邮件中有大量的邮件与嫌疑人有关，那么这个人很有可能也是嫌疑人。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for element in my_dataset:\n",
    "    record = my_dataset[element]\n",
    "    to_messages = record['to_messages']\n",
    "    from_messages = record['from_messages']\n",
    "    from_this_person_to_poi = record['from_this_person_to_poi']\n",
    "    from_poi_to_this_person = record['from_poi_to_this_person']\n",
    "    if  to_messages == 0 or \\\n",
    "    from_messages == 'NaN' or \\\n",
    "    from_this_person_to_poi == 'NaN' or \\\n",
    "    from_poi_to_this_person == 'NaN':\n",
    "        record['poi_email_rate'] = 'NaN'\n",
    "    else:\n",
    "        record['poi_email_rate'] = float(from_this_person_to_poi+from_poi_to_this_person)/(to_messages+from_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次我构造了可以根据特征评分的函数，选取排名靠前的特征进行排序。可以通过百分之多少的比例来控制特征的数量，方便后期调整参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Score:[('exercised_stock_options', 22.484239699561655), ('total_stock_value', 21.924096297344661), ('bonus', 19.158702272676425), ('salary', 16.583003746563765), ('deferred_income', 10.474111674381019), ('long_term_incentive', 9.5538080839163086), ('restricted_stock', 8.6021755972911045), ('total_payments', 8.011715381936348), ('shared_receipt_with_poi', 7.8953356715814165), ('loan_advances', 6.5661652281435723), ('expenses', 5.5798694818050976), ('poi_email_rate', 4.9252304622627996), ('from_poi_to_this_person', 4.9182163726212007), ('other', 4.0899120466075605), ('from_this_person_to_poi', 2.4186632091763709), ('director_fees', 1.8884368442227371), ('to_messages', 1.6505535296368732), ('deferral_payments', 0.2649254972389542), ('from_messages', 0.18041998942131626), ('restricted_stock_deferred', 0.15046529211472459)]\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(percent,print_score = False):\n",
    "    features_list = ['poi',\n",
    "                 'to_messages',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'salary',\n",
    "                 'bonus',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'exercised_stock_options',\n",
    "                 'restricted_stock',\n",
    "                 'shared_receipt_with_poi',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'loan_advances',\n",
    "                 'director_fees',\n",
    "                 'deferred_income',\n",
    "                 'long_term_incentive',\n",
    "                 'other',\n",
    "                 'poi_email_rate']\n",
    "    data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "\n",
    "    cv = StratifiedShuffleSplit(labels, 1000, random_state = 42)\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        selector = SelectPercentile()\n",
    "        selector = selector.fit(features_train,labels_train)\n",
    "        score_list.append(selector.scores_)\n",
    "    \n",
    "    score_average = sum(score_list)/len(score_list)\n",
    "    feature_score = zip(features_list[1:],score_average)\n",
    "    feature_score.sort(key = lambda tup:tup[1],reverse=True)\n",
    "    \n",
    "    new_feature_list = ['poi']\n",
    "    \n",
    "    feature_score = feature_score[:int(len(feature_score)*percent/100)]\n",
    "    \n",
    "    for ele,_ in feature_score:\n",
    "        new_feature_list.append(ele)\n",
    "    \n",
    "    if print_score == True:\n",
    "        print \"Features Score:{}\".format(feature_score)\n",
    "        \n",
    "    return new_feature_list\n",
    "\n",
    "features_list = feature_selection(100, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从得分可以看出poi_email_rate得分较低，所以可能无法作为最终的特征。具体情况需要视算法和特征情况而定。见后文。\n",
    "### 特征缩放\n",
    "使用MinMaxScaler对特征进行缩放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特征缩放函数\n",
    "def feature_scale(features):\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主成因分析\n",
    "选用了PCA函数，进行主成因分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 主成因分析\n",
    "def feature_PCA(features,labels,components_parameter):\n",
    "    pca = PCA(n_components=components_parameter)\n",
    "    features = pca.fit_transform(features,labels)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据转化\n",
    "最后，我构造了一个统一的数据转化函数，可以根据参数对数据调用上述函数对数据特征进行转化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据转化函数\n",
    "def features_transform(features,labels):\n",
    "    features = feature_scale(features) # 特征缩放\n",
    "    features = feature_PCA(features,labels,components_parameter) # 主成因分析\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述是特征工程所需的功能。\n",
    "## 机器学习算法准备\n",
    "本文尝试使用Naive Bayes，Decision Tree，SVM以及Random Forest四种算法，对数据进行分析，首先，我搭建了一个统一的算法函数，可以通过设置参数选择对应的算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearch_test用以控制是否通过GridSearch寻找合适的参数，函数中的各个算法参数都是经过试验获得的，后文会详细展示试验过程。\n",
    "# 通过输入不同的函数名选用不同的参数 \"NB\",\"Decision Tree\",\"Random Forest\",\"SVM\"\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def classifier(algorithm = \"Decision Tree\", GridSearch_test = False):\n",
    "    if algorithm == 'NB':\n",
    "        ## GaussianNB\n",
    "        clf = GaussianNB()\n",
    "    elif algorithm == 'Decision Tree':\n",
    "        ## Decision Tree\n",
    "        clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 2,min_samples_leaf = 9)  \n",
    "        if GridSearch_test:\n",
    "            parameters = {'criterion':[\"entropy\",\"gini\"],'max_depth':(1,10,1),'min_samples_leaf':(1,200,10)}\n",
    "            clf = GridSearchCV(clf,parameters)\n",
    "    elif algorithm == 'Random Forest':\n",
    "        ## Random Forest\n",
    "        clf = RandomForestClassifier(n_estimators = 3)\n",
    "        if GridSearch_test:\n",
    "            parameters = {'n_estimators':[1,10]}\n",
    "            clf = GridSearchCV(clf,parameters)\n",
    "    elif algorithm == 'SVM':\n",
    "        clf = SVC(C=1,gamma = 1)\n",
    "        if GridSearch_test:\n",
    "            parameters = {'C':[0.001, 0.01, 0.1, 1, 10],\n",
    "            \"gamma\":[0.001, 0.01, 0.1, 1]}\n",
    "            clf = GridSearchCV(clf, parameters)\n",
    "    return clf\n",
    "\n",
    "clf = classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法评估准备\n",
    "首先我定义了评估的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 当print_score 为 Ture时，打印算法的accuracy，precision和recall\n",
    "def calculate_score(clf,features,labels,print_score = False):\n",
    "    ## import precision score evaluation\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    accuracy = accuracy_score(clf.predict(features),labels)\n",
    "    precision = precision_score(clf.predict(features),labels)\n",
    "    recall = recall_score(clf.predict(features),labels)\n",
    "    \n",
    "    if print_score:\n",
    "        print clf\n",
    "        print \"accuracy score: {}\".format(accuracy)\n",
    "        print \"precision score: {}\".format(precision)\n",
    "        print \"recall score: {}\".format(recall)\n",
    "    \n",
    "    scores = {\"accuracy\":accuracy,\n",
    "           \"precision\":precision,\n",
    "           \"recall\":recall\n",
    "            }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初我选用了简单的train_test_split，进行算法性能评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "accuracy score: 0.844827586207\n",
      "precision score: 0.428571428571\n",
      "recall score: 0.375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.84482758620689657,\n",
       " 'precision': 0.42857142857142855,\n",
       " 'recall': 0.375}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "clf = classifier()\n",
    "clf = clf.fit(features_train,labels_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "calculate_score(clf,features_test,labels_test,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为样本的不平衡性比较强，所以单一一次的随机状态很容易导致poi分布不均衡，导致训练的模型不够精确。最终的结果与tester中的结果也相差很多。所以我尝试改用kfold，通过多次分组，记录不同组的score，取score平均值进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=2,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=9,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "average accuracy:0.874761904762\n",
      "precision accuracy:0.133333333333\n",
      "recall accuracy:0.25\n"
     ]
    }
   ],
   "source": [
    "# 搭建kfold测试数据组\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(10)\n",
    "cv = kf.split(features)\n",
    "accuracy_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "for train_index,test_index in cv:\n",
    "    features_train = [features[ii] for ii in train_index]\n",
    "    features_test = [features[ii] for ii in test_index]\n",
    "    labels_train = [labels[ii] for ii in train_index]\n",
    "    labels_test = [labels[ii] for ii in test_index]\n",
    "    clf.fit(features_train,labels_train)\n",
    "    score = calculate_score(clf,features_test,labels_test) # 计算每个数据组的数据情况，并且添加到对应list中\n",
    "    accuracy_list.append(score['accuracy'])\n",
    "    precision_list.append(score['precision'])\n",
    "    recall_list.append(score['recall'])\n",
    "# 计算各个list的均值\n",
    "print clf\n",
    "print \"average accuracy:{}\".format(sum(accuracy_list)/len(accuracy_list))\n",
    "print \"precision accuracy:{}\".format(sum(precision_list)/len(precision_list))\n",
    "print \"recall accuracy:{}\".format(sum(recall_list)/len(recall_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的评估效果是会好一些。后来我了解到，对样本容量比较小的数据集，可以使用StratifiedShuffleSplit进行交叉验证。所以我最后复用了tester中的语句进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "\n",
    "\n",
    "PERF_FORMAT_STRING = \"\\\n",
    "\\tAccuracy: {:>0.{display_precision}f}\\tPrecision: {:>0.{display_precision}f}\\t\\\n",
    "Recall: {:>0.{display_precision}f}\\tF1: {:>0.{display_precision}f}\\tF2: {:>0.{display_precision}f}\"\n",
    "RESULTS_FORMAT_STRING = \"\\tTotal predictions: {:4d}\\tTrue positives: {:4d}\\tFalse positives: {:4d}\\\n",
    "\\tFalse negatives: {:4d}\\tTrue negatives: {:4d}\"\n",
    "\n",
    "folds = 1000\n",
    "\n",
    "# \n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "def test_classifier(clf, dataset, feature_list,transform = True):\n",
    "    score_list = []\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        ### feature engineer the features\n",
    "        if transform:\n",
    "            features_train = features_transform(features_train,labels_train)\n",
    "        clf.fit(features_train, labels_train)\n",
    "        ### transform test features\n",
    "        if transform:\n",
    "            features_test = features_transform(features_test,labels_test)\n",
    "        predictions = clf.predict(features_test)\n",
    "        for prediction, truth in zip(predictions, labels_test):\n",
    "            if prediction == 0 and truth == 0:\n",
    "                true_negatives += 1\n",
    "            elif prediction == 0 and truth == 1:\n",
    "                false_negatives += 1\n",
    "            elif prediction == 1 and truth == 0:\n",
    "                false_positives += 1\n",
    "            elif prediction == 1 and truth == 1:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                print \"Warning: Found a predicted label not == 0 or 1.\"\n",
    "                print \"All predictions should take value 0 or 1.\"\n",
    "                print \"Evaluating performance for processed predictions:\"\n",
    "                break\n",
    "\n",
    "    try:\n",
    "        \n",
    "        total_predictions = true_negatives + false_negatives + false_positives + true_positives\n",
    "        accuracy = 1.0*(true_positives + true_negatives)/total_predictions\n",
    "        precision = 1.0*true_positives/(true_positives+false_positives)\n",
    "        recall = 1.0*true_positives/(true_positives+false_negatives)\n",
    "        f1 = 2.0 * true_positives/(2*true_positives + false_positives+false_negatives)\n",
    "        f2 = (1+2.0*2.0) * precision*recall/(4*precision + recall)\n",
    "        print clf\n",
    "        print PERF_FORMAT_STRING.format(accuracy, precision, recall, f1, f2, display_precision = 5)\n",
    "        print RESULTS_FORMAT_STRING.format(total_predictions, true_positives, false_positives, false_negatives, true_negatives)\n",
    "        print \"\"\n",
    "    except:\n",
    "        print \"Got a divide by zero when trying out:\", clf\n",
    "        print \"Precision or recall may be undefined due to a lack of true positive predicitons.\"\n",
    "\n",
    "test_classifier(clf, my_dataset, features_list,transform = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StratifiedShuffleSplit会对数据集进行打乱并分层，充分的随机性可以满足对不平衡数据集的验证，耗费的计算会相对比较多，但是因为样本容量比较小，所以也可以接受。这样可以确保训练出来的分类器可以符合数据集实际的状态，减少在不平衡的数据情况下训练，导致无法准确分类的情况。\n",
    "## 参数调整\n",
    "接下来，我开始对整个模型进行参数调整。首先，我使用GridSearchCV，对各种算法的参数进行了调整：\n",
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = classifier(\"Decision Tree\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=2,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.85980\tPrecision: 0.01869\tRecall: 0.00100\tF1: 0.00190\tF2: 0.00123\n",
      "\tTotal predictions: 15000\tTrue positives:    2\tFalse positives:  105\tFalse negatives: 1998\tTrue negatives: 12895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 1,min_samples_leaf = 2)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False,add = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "clf = classifier(\"Random Forest\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=2, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.85120\tPrecision: 0.31703\tRecall: 0.10050\tF1: 0.15262\tF2: 0.11640\n",
      "\tTotal predictions: 15000\tTrue positives:  201\tFalse positives:  433\tFalse negatives: 1799\tTrue negatives: 12567\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 2)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False,add = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "clf = classifier(\"SVM\",GridSearch_test = True)\n",
    "clf.fit(features_train,labels_train)\n",
    "print clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got a divide by zero when trying out: SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "Precision or recall may be undefined due to a lack of true positive predicitons.\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=0.001,gamma=0.001)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = False,add = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述的几次测试来看GridSearch都没有达到比较好的效果。所以我最后采用了手动调整的方法，手动调整各个算法中的参数，以及选用特征的比例（feature_selection：selector_percentile），和构造新特征的个数（PCA：components）。\n",
    "### 手动调整\n",
    "手动调整算法参数以及特征参数的结果如下：\n",
    "#### Decision Tree\n",
    "criterion = \"entropy\",max_depth = 20,min_samples_leaf = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exercised_stock_options', 22.484239699561655), ('total_stock_value', 21.924096297344661), ('bonus', 19.158702272676425), ('salary', 16.583003746563765), ('deferred_income', 10.474111674381019)]\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=20,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=3,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "\tAccuracy: 0.79621\tPrecision: 0.32281\tRecall: 0.38850\tF1: 0.35262\tF2: 0.37331\n",
      "\tTotal predictions: 14000\tTrue positives:  777\tFalse positives: 1630\tFalse negatives: 1223\tTrue negatives: 10370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 2\n",
    "percent = 25\n",
    "features_list = feature_selection(percent,print_score = True)\n",
    "clf = DecisionTreeClassifier(criterion = \"entropy\",max_depth = 20,min_samples_leaf = 3)  \n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "最终得出参数components_parameter:1, percent:35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "\tAccuracy: 0.78329\tPrecision: 0.31885\tRecall: 0.45500\tF1: 0.37495\tF2: 0.41920\n",
      "\tTotal predictions: 14000\tTrue positives:  910\tFalse positives: 1944\tFalse negatives: 1090\tTrue negatives: 10056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 1\n",
    "percent = 35\n",
    "clf = clf = GaussianNB()\n",
    "features_list = feature_selection(percent)\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "最终得出参数components_parameter:1, percent:20 n_estimators = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exercised_stock_options', 22.484239699561655), ('total_stock_value', 21.924096297344661), ('bonus', 19.158702272676425), ('salary', 16.583003746563765)]\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=3, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "\tAccuracy: 0.79415\tPrecision: 0.32192\tRecall: 0.30550\tF1: 0.31349\tF2: 0.30865\n",
      "\tTotal predictions: 13000\tTrue positives:  611\tFalse positives: 1287\tFalse negatives: 1389\tTrue negatives: 9713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 1\n",
    "percent = 20\n",
    "clf = RandomForestClassifier(n_estimators = 3)\n",
    "features_list = feature_selection(percent,print_score = True)\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('exercised_stock_options', 22.484239699561655)]\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "\tAccuracy: 0.88473\tPrecision: 0.37796\tRecall: 0.41500\tF1: 0.39561\tF2: 0.40702\n",
      "\tTotal predictions: 11000\tTrue positives:  415\tFalse positives:  683\tFalse negatives:  585\tTrue negatives: 9317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "components_parameter = 1\n",
    "percent = 5\n",
    "clf = SVC(kernel = 'rbf',C=1,gamma = 1)\n",
    "features_list = feature_selection(percent,print_score = True)\n",
    "test_classifier(clf, my_dataset, features_list,transform = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后整体调整参数可得SVM这个算法是最优的算法。\n",
    "其参数为：  \n",
    "- components_parameter = 1  \n",
    "- percent = 5  \n",
    "- C=1  \n",
    "- gamma = 1  \n",
    "\n",
    "选用了特征'exercised_stock_options'，其评分为22.484239699561655。新增的特征没有纳入特征选择范围。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题总结\n",
    "\n",
    "- 你在获得数据时它们是否包含任何异常值，你是如何进行处理的？【相关标准项：“数据探索”，“异常值调查”】\n",
    "\n",
    "包含了部分异常值，一部分异常值是数据统计时产生的异常值，如'TOTAL'的值，直接删除。  \n",
    "还有非人名的值，也进行了删除，如'THE TRAVEL AGENCY IN THE PARK'  \n",
    "最后，对全部都是NaN的值，也进行了删除。如'LOCKHART EUGENE E'    \n",
    "\n",
    "- 你最终在你的 POI 标识符中使用了什么特征，你使用了什么筛选过程来挑选它们？你是否需要进行任何缩放？为什么？作为任务的一部分，你应该尝试设计自己的特征，而非使用数据集中现成的——解释你尝试创建的特征及其基本原理。（你不一定要在最后的分析中使用它，而只设计并测试它）。在你的特征选择步骤，如果你使用了算法（如决策树），请也给出所使用特征的特征重要性；如果你使用了自动特征选择函数（如 SelectBest），请报告特征得分及你所选的参数值的原因。【相关标准项：“创建新特征”、“适当缩放特征”、“智能选择功能”】\n",
    "\n",
    "首先我添加了尽可能多的参数，可以保证在算法训练中使用到尽可能多的信息。接下来我通过SelectPercentile来选择参数，这样可以方便调节选用哪些参数，提升或减少用以训练的信息量。  \n",
    "我根据邮件数总量(发送的邮件和收到的邮件之和)，以及与poi有关的邮件量（发自poi的邮件和发送给poi的邮件之和），创建了嫌疑人邮件率的特征。我认为如果一个人他的嫌疑人邮件率比较高，那么说明他很有可能是一个嫌疑人。  \n",
    "我使用了PCA进行了主成因分析，并且用MinMax进行了特征缩放，根据调参的结果，这样做可以提升算法性能。 \n",
    "\n",
    "- 你最终使用了什么算法？你还尝试了其他什么算法？不同算法之间的模型性能有何差异？【相关标准项：“选择算法”】  \n",
    "\n",
    "我最终使用了SVM，我还尝试了Naive Bayes、Random Forest以及Decision Tree。Naive Bayes召回率最高，但是精准度难以提升。Random Forest通过调整可以有很高的正确率和召回率，但是相对耗时偏长。Decision Tree在使用Entrophy时，无需调整特征就可以达到很高的精准度和召回率。但是总和评判下来，SVM拥有最高的分类准确度、正确率、召回率，而且预测时间也可以接受。  \n",
    "\n",
    "- 调整算法的参数是什么意思，如果你不这样做会发生什么？你是如何调整特定算法的参数的？（一些算法没有需要调整的参数 – 如果你选择的算法是这种情况，指明并简要解释对于你最终未选择的模型或需要参数调整的不同模型，例如决策树分类器，你会怎么做）。【相关标准项：“调整算法”】 \n",
    "  \n",
    "我使用了GridSearcCV进行调参，但是我发现会出现过度拟合的情况。可能的问题是样本量设置过大，导致过度拟合。最后我是通过手动调参，并且记录的方式进行调整，这样做可以结合precision和recall的情况对参数做目标性的调整，效率可以更加高。  \n",
    "\n",
    "- 什么是验证，未正确执行情况下的典型错误是什么？你是如何验证你的分析的？【相关标准项：“验证策略”】  \n",
    "\n",
    "典型错误就是过度拟合，在样本中表现非常好，但是在测试过程中表现就不太好。遇到这样的问题，我会适当减少训练算法的深度，减少拟合的水平。  \n",
    "\n",
    "- 给出至少 2 个评估度量并说明每个的平均性能。解释对用简单的语言表明算法性能的度量的解读。【相关标准项：“评估度量的使用”】\n",
    "\n",
    "precision rate：预测为嫌疑人中真正是嫌疑人的占比。即测的对不对。  \n",
    "recall rate：所有嫌疑人被识别出的人的占比。即测的全不全。  \n",
    "\n",
    "以上是对数据集的分析。其实还有很多问题没有解决：  \n",
    "\n",
    "- 如何通过机器的方法自动调整包括特征工程在内的参数  \n",
    "- 是否有其他算法，比文中所选的算法更加有效  \n",
    "- 是否在数据点足够多的情况下，是否要将时间也考虑在内  \n",
    "\n",
    "## 关于代码\n",
    "本文中覆盖了所有与结果相关的代码。随文附上调参时使用的代码，参数均已设置为最优算法与参数。为了能够体现特征工程的效果，我在tester文件中添加了特征工程的相关功能，供测试使用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
